<!-- # The Foundation of LLM Training: Cost, Speed, and MFU

LLM을 훈련하는 것은 상당한 비용과 시간이 드는 작업이며, 특히 모델의 크기가 클수록 더욱 그렇습니다. 따라서 훈련 과정의 효율성을 극대화하는 것이 매우 중요합니다. 효율성의 핵심 지표 중 하나는 바로 MFU(Model FLOPs Utilization)입니다. 이번 글에서는 MFU가 무엇인지, 왜 중요한지, 그리고 어떻게 MFU를 최적화하여 훈련 비용과 시간을 절약할 수 있는지 알아보겠습니다.


## MFU란 무엇인가, 왜 중요한가?

연구원들과 엔지니어들이 새로운 모델 구조나 학습 기법을 도입할 때, 주로 모델 downstream task 성능에 집중합니다. 그러나 대규모 학습 환경에서는 이러한 지표만큼 중요한 것이 바로 연산 효율성, 즉 FLOPs가 실제로 얼마나 활용되고 있는가입니다. 모델의 성능을 1\~2% 끌어올리기 위해 전체 학습 시간을 20% 더 늘려야 한다면, 그것은 진정한 개선일까요?

많은 최신 논문에서 제안되는 테크닉들이 실제 대규모 모델 학습 환경에서 학습 효율성의 저하로 인해 버려집니다. 예를 들어 작은 모듈을 모델 내부에 삽입하거나 학습 중 그 모델에서 나온 결과를 활용하는 방식(rho1, byte latent model 등)들은 같은 학습 규모에서는 성능이 좋을 수 있으나 분산 환경에서 오버헤드로 작용하여 결국 실질적으로는 나쁜 성능을 내는 결과를 낳기도 합니다.


## 실제 환경에서의 MFU 수준: 잘하고 있다는 기준은?

현실적으로 완전한 MFU 100%는 불가능합니다. 따라서 우리는 하드웨어 종류별로 “어느 정도면 잘하고 있는 것인가?”라는 판단 기준이 필요합니다.

예를 들어, H100 GPU 기반의 멀티 노드 환경에서 Dense 모델의 MFU가 40% 이상이라면 이미 상당히 효율적인 학습이 이뤄지고 있는 것으로 평가할 수 있습니다. A100 환경에서는 이 기준이 55% 수준으로 약간 더 높습니다. 이 수치들은 실제로 대규모 학습에서 병목 없이 데이터가 흐르고 통신 및 연산이 조화롭게 작동하고 있을 때 달성 가능한 값들입니다.

따라서 현재 운영하고 있는 시스템의 MFU가 위 기준에 근접해 있다면, 더 이상의 최적화보다는 데이터 품질이나 모델 설계의 실험성으로 초점을 옮기는 것이 비용 효율적일 수 있습니다.  


## MFU 측정은 어떻게 하는가?

MFU는 다음 공식으로 계산할 수 있습니다:
```
MFU = (실제 수행된 FLOPs / 학습에 걸린 시간) ÷ (GPU의 최대 TFLOPS)
```
예를 들어 1 step에 총 624 TFLOPs를 사용했고 4초 걸렸다면, 초당 156 TFLOPS의 연산이 일어난 셈입니다. 만약 해당 GPU의 datasheet에서 연산량 최대치가 312 TFLOPS라면,
```
MFU = 156 / 312 = 0.5 → 50%
```

GPU의 최대 성능 대비 50%를 실제로 활용하고 있다는 뜻입니다.



## MFU를 보는 눈 – FLOPs와 친해지기

## 간단한 계산으로 동료들을 감동시키기

Optional
!Distilation 의 경우
inference가 동반된다. 
mfu 50% 등을 뽑아내기 쉽지않다. 왜냐하면 prefil과 decoding으로 나뉘기 때문이다. 스토리지가 충분하다면 미리 오랫동안 저비용 gpu로 뽑아놀 수도있다. 
https://arxiv.org/pdf/2502.08606

MLA의 경우
mla는 attention에 들어간다. 그래서 ffn에서 파생되는 첫번째항은 변하지 않고 두번째, 세번째 항을 바꾸면 된다. 


## FLOPs 구조 분석: D²와 D·H, 그리고 그 계수들

Transformer 모델의 FLOPs는 크게 두 구성으로 나뉩니다: 하나는 Feedforward Layer(MLP 등)에 해당하는 D² 항, 다른 하나는 Attention에 해당하는 D·H 항입니다. 많은 기술 문서에서는 이를 단순히 “FLOPs는 대략 D²에 비례한다”고 정리하지만, 실전에서는 계수의 차이가 매우 중요합니다.

예를 들어, 다음은 실제 모델 구조에서 각 항의 계수가 어떻게 달라지는지 정리한 비교입니다.

| FFN 구조 | 확장비(r) | FFN 계수 | Attention 구조    | K/N | Proj 계수 | 총 FLOPs 계수 |
| ------ | ------ | ------ | --------------- | --- | ------- | ---------- |
| SwigLU | 8/3    | 48     | GQA (32q, 8kv)  | 1/4 | 15      | 63     |
| SwigLU | 8/3    | 48     | MHA (32q, 32kv) | 1   | 24      | 72     |
| SwigLU | 4      | 72     | MHA             | 1   | 24      | 96     |
| GELU   | 4      | 48     | MHA             | 1   | 24      | 72     |
| GELU   | 1      | 12     | MHA             | 1   | 24      | 36     |

이 비교를 통해 알 수 있듯, 같은 D 값을 유지하더라도 구조에 따라 실제 FLOPs는 크게 달라질 수 있습니다. 특히 FFN 구조에서의 확장비(r)를 조정하는 것이 FLOPs에 미치는 영향은 상당히 크며, Attention 구조 변경에 따른 효과보다도 훨씬 큽니다.

실제로 Qwen2 등의 모델이 SwigLU를 도입하면서 확장비를 정확히 8/3으로 조정한 이유는 FLOPs를 불필요하게 키우지 않으면서도 충분한 표현력을 확보하기 위한 결과입니다.



## MFU를 높이기 위한 실용적 전략

실제 MFU를 높이기 위한 방법은 여러 가지가 있습니다. 모델 구조 최적화, 분산 학습 환경의 통신 효율 개선, 데이터 파이프라인의 병목 제거, 하드웨어 친화적인 설정 등이 모두 포함됩니다. 그러나 위에서 설명한 FLOPs 분석 관점에서 본다면, FFN 구조와 Attention 구조의 FLOPs 계수에 대한 감각을 갖추는 것이 가장 실질적인 출발점입니다.

즉, 새로운 구조나 논문을 도입하기 전에, 그것이 MFU에 어떤 영향을 미칠지를 판단할 수 있는 훈련이 필요합니다. GQA로 전환했을 때 FLOPs가 얼마나 줄어드는가? SwigLU의 확장비를 바꾸면 FLOPs는 얼마나 늘어나는가? 이런 감각은 수치상의 계산을 넘어서, 전체 시스템 효율에 대한 직관적인 이해로 이어집니다.

또한, 이를 기반으로 한 모델 구조 비교 실험 시 단순 성능 수치가 아닌 FLOPs 대비 성능 효율(FLOPs/accuracy ratio 등)을 함께 비교해야 더 실용적인 결정을 내릴 수 있습니다.


## 결론

MFU는 단순한 지표가 아닙니다. 그것은 대규모 학습 전략을 어떻게 설계할 것인가에 대한 핵심 기준이며, 성능과 효율성 사이의 균형을 정량적으로 판단할 수 있는 프레임워크입니다. 본 가이드에서 소개한 D²와 D·H 항의 FLOPs 분석을 통해, 독자는 모델 구조를 설계하거나 선택할 때 MFU 관점에서 사전 판단을 내릴 수 있는 기반을 마련할 수 있을 것입니다. 실전에서는 완벽한 MFU를 목표로 삼기보다, 어떤 선택이 전체 시스템 효율을 가장 잘 높이는 방향인지를 항상 고민해야 합니다. 그런 면에서 MFU는 단순한 수치가 아니라, 실용적 사고방식 그 자체이기도 합니다.



# MFU를 보는 눈 – FLOPs와 친해지기

이 글은 Model FLOPs Utilization(MFU)에 대해 많이 들어봤지만, 그 숫자 하나가 왜 중요한지, 그리고 진짜 연산량 계산이 어떻게 이뤄지는지 감이 잘 안 잡히는 사람들을 위한 현실적인 이야기다. 특히 중급 이상 엔지니어로서 “어느 정도 학습이 잘 되고 있는지”를 판단하고 싶을 때, MFU는 그저 ‘좋아 보이는 수치’가 아니라 진짜 중요한 판단 도구가 될 수 있다. 문제는 대부분 MFU가 뭔지는 아는데, 어떻게 계산되는지, 구조가 어떻게 연산량에 영향을 주는지는 모른다는 것.

## 이 숫자 하나로 많은 걸 판단할 수 있다

누군가 GPU를 수백 개 붙여놓고 하루 10억 토큰씩 학습하고 있다고 해도, MFU가 20%라면 사실상 리소스를 절반도 못 쓰고 있는 셈이다. 반대로, 40\~50% 이상의 MFU를 유지하는 환경이라면 설계도 잘 되어 있고, 코드도 최적화되어 있으며, 네트워크와 IO도 뒷받침되고 있다는 신호다.

그러니까 MFU는 단순히 “속도 빠르냐”의 문제가 아니라, 전체 파이프라인이 잘 동작하고 있는지 보는 하나의 창문 같은 거다.


## FLOPs를 이해하면 MFU가 보인다

많은 사람들이 TFLOPS 수치를 보고 “이 GPU 진짜 빠르네”라고 생각한다. 하지만 중요한 건 그 TFLOPS 중 실제로 얼마나 활용되었냐는 거고, 이게 바로 MFU다. 계산은 간단하다:

> MFU = (실제 연산한 FLOPs / 소요 시간) ÷ (광고된 Peak TFLOPS)

이걸 이해하려면, 먼저 “모델 구조가 얼마나 많은 FLOPs를 발생시키는가”를 알아야 한다. 여기서 대부분 실수를 한다. D² 항이 크다는데, 얼마나 큰지 직접 계산해본 사람은 많지 않다.


## 모델 구조가 FLOPs를 어떻게 바꾸는가

Transformer 구조는 FLOPs의 절반 이상을 D²에 몰아넣는다. 특히 FFN(MLP), 그리고 QKVO projection이 여기에 해당된다. 그 외의 연산 (LayerNorm, GELU) 이런 건 Bsz * SeqLen * D 크기이기 때문에 FLops에는 큰 영향을 주지 않는다.

재밌는 건 D² 앞에 붙는 계수가 생각보다 다이나믹하다는 점이다. 예를 들어 같은 D를 쓰더라도 SwigLU와 GELU의 차이, GQA와 MHA의 차이, FFN 확장 계수(r)의 차이만으로도 FLOPs는 수십 퍼센트가 왔다갔다 한다.

### 예시를 하나 들어보자

아래는 FFN 구조와 Attention 구조에 따라 D² 앞 계수가 어떻게 바뀌는지를 정리한 표다:

| FFN 구조 | 확장비 | FFN 계수 | Attention 구조    | K/N 비율 | Proj 계수 | 총 D² 계수 |
| ------ | --- | ------ | --------------- | ------ | ------- | ------- |
| SwigLU | 8/3 | 48     | GQA (32q, 8kv)  | 1/4    | 15      | 63  |
| SwigLU | 8/3 | 48     | MHA (32q, 32kv) | 1      | 24      | 72  |
| SwigLU | 4   | 72     | MHA             | 1      | 24      | 96  |
| GELU   | 4   | 48     | MHA             | 1      | 24      | 72  |
| GELU   | 1   | 12     | MHA             | 1      | 24      | 36  |

SwigLU로 FFN을 구성하고 GQA를 쓰면 D² 항 계수는 63. 하지만 같은 SwigLU라도 MHA를 쓰면 72로 올라가고, 확장비를 4로 키우면 무려 96까지 치솟는다. 이건 거의 1.5배 차이다. FLOPs도, MFU도 당연히 영향을 받는다.

---

## SwigLU가 대세가 된 이유?

Qwen2 같은 모델들이 SwigLU를 도입한 이유도 결국은 여기에 있다. GELU보다 SwigLU가 FLOPs를 동일하게 유지하면서도 성능은 좋아지는 구조였기 때문이다. 게다가 확장비를 8/3으로 깔끔하게 맞춰서 FLOPs를 예측 가능하게 설계한 것도 크다. 계산량을 고정시키면서도 구조적으로 표현력을 확보하려는 시도라고 보면 된다. 이건 단순히 성능 튜닝이 아니라 계산량 자체를 세심하게 다루는 설계의 기술이다. 이런 감각을 익히는 게 결국 학습 효율성을 높이는 데도 도움이 된다.


## 실전에서 MFU 기준은 어디까지?

많은 사람들이 “그럼 몇 %면 잘하는 거냐”고 묻는데, 이건 하드웨어마다 다르다.

* H100 기준: 40% 넘으면 잘하고 있음
* A100 기준: 50\~55%면 상당히 좋은 편

이 수치는 그냥 뽑은 게 아니라 실제 대규모 학습 환경에서 나온 체감 기준이다. 더 나아가려면 커널을 커스터마이징하거나, 통신과 연산을 더 타이트하게 겹쳐야 한다. 그리고 거기서부터는 거의 아트다.

## MFU를 높이기 위한 감각

가장 빠른 방법은 사실 구조적인 감각을 기르는 것이다.
예를 들어:

* GQA를 쓰면 Attention projection의 D² 계수가 24 → 15로 감소
* FFN 확장비를 4 → 8/3으로 줄이면 FFN FLOPs도 줄어듦
* Proj 계수가 낮아지면 모델 파라미터 수는 큰 차이 없지만 FLOPs는 줄어듦

즉, 연산량은 수식이지만, 그 수식 안에 있는 계수들이 의미하는 바는 물리적 비용이다. 그걸 체감하는 훈련이 필요하다. 이걸 알고 있어야 모델을 설계하거나 논문을 읽을 때 “이 구조는 FLOPs가 얼마나 무거울까?”를 자연스럽게 추론할 수 있다.


## 계산을 넘어서 감각으로

결국 MFU는 단순한 계산 공식이 아니다. 연산량에 대한 직관, 모델 구조와 FLOPs의 연관성, 하드웨어 특성에 대한 이해 – 이 모든 걸 종합한 결과물이다. 그리고 이것이 잘 되어 있을 때, 모델은 단순히 ‘빠르게’ 학습되는 것을 넘어서 ‘의미 있는 방식으로 효율적으로’ 학습된다.

지금 내가 보는 FLOPs가 어떤 의미를 가지는지, 그리고 그 수치가 모델 구조 변경에 따라 어떻게 반응하는지를 볼 줄 안다면, 당신은 이미 MFU라는 창을 통해 학습 시스템 전체를 보는 눈을 갖추고 있는 것이다.