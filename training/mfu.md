# The Foundation of LLM Training: Cost, Speed, and MFU

LLM을 훈련하는 것은 상당한 비용과 시간이 드는 작업이며, 특히 모델의 크기가 클수록 더욱 그렇습니다. 따라서 훈련 과정의 효율성을 극대화하는 것이 매우 중요합니다. 효율성의 핵심 지표 중 하나는 바로 MFU(Model FLOPs Utilization)입니다. 이번 글에서는 MFU가 무엇인지, 왜 중요한지, 그리고 어떻게 MFU를 최적화하여 훈련 비용과 시간을 절약할 수 있는지 이론적으로 알아보겠습니다.


## MFU란 무엇인가, 왜 중요한가?

연구원들과 엔지니어들이 새로운 모델 구조나 학습 기법을 도입할 때, 주로 모델 downstream task 성능에 집중합니다. 그러나 대규모 학습 환경에서는 이러한 지표만큼 중요한 것이 바로 연산 효율성, 즉 FLOPs가 실제로 얼마나 활용되고 있는가입니다. 모델의 성능을 1\~2% 끌어올리기 위해 전체 학습 시간을 20% 더 늘려야 한다면, 그것은 진정한 개선일까요?

많은 최신 논문에서 제안되는 테크닉들이 실제 대규모 모델 학습 환경에서 학습 효율성의 저하로 인해 버려집니다. 예를 들어 작은 모듈을 모델 내부에 삽입하거나 학습 중 그 모델에서 나온 결과를 활용하는 방식들은 같은 학습 규모에서는 성능이 좋을 수 있으나 분산 환경에서 오버헤드로 작용하여 결국 실질적으로는 나쁜 성능을 내는 결과를 낳기도 합니다.


MFU는 다음 공식으로 계산할 수 있습니다:
```
MFU = (실제 수행된 FLOPs / 학습에 걸린 시간) ÷ (GPU의 최대 TFLOPS)
```
예를 들어 1 step에 총 624 TFLOPs를 사용했고 4초 걸렸다면, 초당 156 TFLOPS의 연산이 일어난 셈입니다. 만약 해당 GPU의 datasheet에서 연산량 최대치가 312 TFLOPS라면,
```
MFU = 156 / 312 = 0.5 → 50%
```

GPU의 최대 성능 대비 50%를 실제로 활용하고 있다는 뜻입니다.


## 실제 환경에서의 MFU 수준: 잘하고 있다는 기준은?

현실적으로 완전한 MFU 100%는 불가능합니다. 따라서 우리는 하드웨어 종류별로 “어느 정도면 잘하고 있는 것인가?”라는 판단 기준이 필요합니다.

예를 들어, H100 GPU 기반의 멀티 노드 환경에서 Dense 모델의 MFU가 40% 이상이라면 이미 상당히 효율적인 학습이 이뤄지고 있는 것으로 평가할 수 있습니다. A100 환경에서는 이 기준이 55% 수준으로 약간 더 높습니다. 이 수치들은 실제로 대규모 학습에서 병목 없이 데이터가 흐르고 통신 및 연산이 조화롭게 작동하고 있을 때 달성 가능한 값들입니다.

따라서 현재 운영하고 있는 시스템의 MFU가 위 기준에 근접해 있다면, 더 이상의 최적화보다는 데이터 품질이나 모델 설계의 실험성으로 초점을 옮기는 것이 비용 효율적일 수 있습니다.  



## MFU를 보는 눈 – FLOPs와 친해지기

Transformer 모델의 FLOPs는 크게 두 구성으로 나뉩니다: 하나는 Feedforward Layer와 QKVO projection에 해당하는 D² 항, 다른 하나는 Attention에 해당하는 D·H 항입니다. 많은 기술 문서에서는 이를 단순히 “FLOPs는 대략 D²에 비례한다”고 정리하지만, 실전에서는 계수의 차이가 매우 중요합니다.

예를 들어, 다음은 실제 모델 구조에서 각 항의 계수가 어떻게 달라지는지 정리한 비교입니다.

| FFN 구조 | ffn multiplier(r) | FFN 계수 | Attention 구조    | K/N | Proj 계수 | 총 FLOPs 계수 |
| ------ | ------ | ------ | --------------- | --- | ------- | ---------- |
| SwigLU | 8/3    | 48     | GQA (32q, 8kv)  | 1/4 | 15      | 63     |
| SwigLU | 8/3    | 48     | MHA (32q, 32kv) | 1   | 24      | 72     |
| SwigLU | 4      | 72     | MHA             | 1   | 24      | 96     |
| GELU   | 4      | 48     | MHA             | 1   | 24      | 72     |
| GELU   | 1      | 12     | MHA             | 1   | 24      | 36     |

이 비교를 통해 알 수 있듯, 같은 D 값을 유지하더라도 구조에 따라 실제 FLOPs는 크게 달라질 수 있습니다. 특히 FFN 구조에서의 ffn multiplier(r)를 조정하는 것이 FLOPs에 미치는 영향은 상당히 크며, Attention 구조 변경에 따른 효과보다도 훨씬 큽니다.

실제로 Qwen2 등의 모델이 SwigLU를 도입하면서 ffn multiplier를 8/3으로 조정한 이유는 FLOPs를 불필요하게 키우지 않으면서도 충분한 표현력을 확보하기 위한 결과입니다.

`flops.py` 에서 각 모델 구조별 flops를 계산하는 코드를 작성해두었습니다.


## MFU를 높이기 위한 실용적 전략

실제 MFU를 높이기 위한 방법은 여러 가지가 있습니다. 모델 구조 최적화, 분산 학습 환경의 통신 효율 개선, 데이터 파이프라인의 병목 제거, 하드웨어 친화적인 설정 등이 모두 포함됩니다. 그러나 위에서 설명한 FLOPs 분석 관점에서 본다면, FFN 구조와 Attention 구조의 FLOPs 계수에 대한 감각을 갖추는 것이 가장 실질적인 출발점입니다.

즉, 새로운 구조나 논문을 도입하기 전에, 그것이 MFU에 어떤 영향을 미칠지를 판단할 수 있는 훈련이 필요합니다. GQA로 전환했을 때 FLOPs가 얼마나 줄어드는가? FFN Layer를 MoE 구조로 FLOPs는 얼마나 늘어나는가? 이런 감각은 수치상의 계산을 넘어서, 전체 시스템 효율에 대한 직관적인 이해로 이어집니다.

또한, 이를 기반으로 한 모델 구조 비교 실험 시 단순 성능 수치가 아닌 FLOPs 대비 성능 효율을 함께 비교해야 더 실용적인 결정을 내릴 수 있습니다.